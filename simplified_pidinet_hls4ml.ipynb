{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6166c9eb",
   "metadata": {},
   "source": [
    "# Simplified Keras/QKeras PiDiNet for hls4ml\n",
    "\n",
    "**Objective:** Build, train (float and quantized), and prepare a simplified Keras PiDiNet for hls4ml synthesis.\n",
    "\n",
    "**Motivation:** The original PyTorch PiDiNet implementation uses custom Pixel Difference Convolution (PDC) operations that are not directly supported by standard deep learning frameworks like Keras or quantization toolkits like QKeras, nor by hardware synthesis tools like hls4ml. To deploy PiDiNet on FPGAs using hls4ml, we need a version representable with standard layers.\n",
    "\n",
    "**PiDiNet Philosophy:** The original PiDiNet workflow involves training a model with PDC operators and then *converting* the learned weights into equivalent standard convolutional kernels before deployment. This conversion captures the edge-detection properties learned by the PDC operators within standard convolution weights.\n",
    "\n",
    "**Keras/hls4ml Approach:** Due to framework limitations (lack of direct PDC support in Keras/QKeras, hls4ml compatibility), we will adopt a slightly different approach. Instead of training with PDC and then converting, we will **build the Keras/QKeras models directly using the target vanilla CNN structure**. The conversion logic from the original PDC types (cv, cd, ad, rd) will be incorporated by choosing the appropriate `kernel_size` and `padding` for standard `Conv2D`/`QConv2D` layers during the model definition phase. This effectively pre-converts the *architecture* to its standard CNN equivalent before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e1215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "import qkeras\n",
    "from qkeras import QConv2D, QActivation, QDense, quantized_bits\n",
    "import hls4ml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import tensorflow_datasets as tfds # Example dataset loader\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"QKeras version: {qkeras.__version__}\")\n",
    "print(f\"hls4ml version: {hls4ml.__version__}\")\n",
    "\n",
    "# Configure GPU (optional)\n",
    "# IMPORTANT: After installing/changing TensorFlow version (e.g., downgrading),\n",
    "# re-run this cell and carefully check its output to ensure the GPU is still detected.\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Configure memory growth (good practice)\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        # Check if GPUs were found\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU detected. Check TensorFlow installation and CUDA compatibility.\")\n",
    "    print(\"Note: QKeras training in Cell 6 requires a GPU due to grouped convolutions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e64211",
   "metadata": {},
   "source": [
    "## Cell 2: Define Keras PiDiNet Architecture (Vanilla Structure)\n",
    "\n",
    "This model uses standard Keras layers (`Conv2D`, `BatchNormalization`, etc.). The key is that the parameters for the depthwise-like `Conv2D` layers (kernel size, padding) are chosen based on the *original* PDC type they represent, effectively building the converted structure directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ae23ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Constants ---\n",
    "NUM_STAGES = 2 # Simplified example\n",
    "BLOCKS_PER_STAGE = 2 # Simplified example\n",
    "INITIAL_FILTERS = 16 # Corresponds to 'C' in PiDiNet paper for micro/tiny\n",
    "DIL_FACTOR = 4 # Example dilation factor if CDCM were used\n",
    "USE_SA = True # Example flag for CSAM\n",
    "USE_DIL = True # Example flag for CDCM\n",
    "\n",
    "# Simplified PDC config list for 2 stages, 2 blocks each + init_block\n",
    "# Example: ['cd', 'cd', 'cd', 'ad', 'ad'] # init + 2*2 blocks\n",
    "PDC_CONFIG = ['cd', 'cd', 'cd', 'ad', 'ad']\n",
    "INPUT_SHAPE = (128, 128, 3) # Example input shape\n",
    "NUM_CLASSES = 1 # For edge detection (binary)\n",
    "\n",
    "# --- Helper Function: Get Standard Conv Params from PDC Type ---\n",
    "def get_pdc_params(original_pdc_type):\n",
    "    \"\"\"Maps original PDC type to kernel_size/padding for standard Conv2D.\"\"\"\n",
    "    if original_pdc_type == 'rd':\n",
    "        return {'kernel_size': 5, 'padding': 2}\n",
    "    elif original_pdc_type in ['cv', 'cd', 'ad']:\n",
    "        return {'kernel_size': 3, 'padding': 1}\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown PDC type: {original_pdc_type}\")\n",
    "\n",
    "# --- Keras PDC Block (Converted Structure) ---\n",
    "def PDCBlock_converted_keras(x, original_pdc_type, inplane, ouplane, stride=1):\n",
    "    \"\"\"Implements the PDC block using standard Keras layers.\"\"\"\n",
    "    identity = x\n",
    "\n",
    "    # Shortcut connection\n",
    "    if stride > 1:\n",
    "        x = layers.MaxPooling2D(pool_size=2, strides=2)(x)\n",
    "        identity = layers.Conv2D(ouplane, kernel_size=1, strides=1, padding='same', use_bias=False)(x)\n",
    "    elif inplane != ouplane:\n",
    "        identity = layers.Conv2D(ouplane, kernel_size=1, strides=1, padding='same', use_bias=False)(x)\n",
    "    # else: identity remains x (stride=1, same channels)\n",
    "\n",
    "    # Main path\n",
    "    pdc_params = get_pdc_params(original_pdc_type)\n",
    "    # Depthwise-like conv based on PDC type\n",
    "    y = layers.Conv2D(inplane, # Depthwise uses input channels\n",
    "                       kernel_size=pdc_params['kernel_size'],\n",
    "                       strides=1, # Stride applied in MaxPool\n",
    "                       padding=pdc_params['padding'],\n",
    "                       groups=inplane, # Simulate depthwise\n",
    "                       use_bias=False)(x)\n",
    "    # Note: Original PiDiNet might have BN/ReLU here, simplified for clarity\n",
    "    y = layers.ReLU()(y)\n",
    "    # Pointwise conv\n",
    "    y = layers.Conv2D(ouplane, kernel_size=1, strides=1, padding='same', use_bias=False)(y)\n",
    "    # Note: Original PiDiNet might have BN/ReLU here too\n",
    "\n",
    "    # Residual connection\n",
    "    out = layers.add([identity, y])\n",
    "    # Optional: Add final ReLU/BN if needed\n",
    "    # out = layers.ReLU()(out)\n",
    "    return out\n",
    "\n",
    "# --- Define Simplified Keras PiDiNet Model ---\n",
    "def build_simplified_keras_pidinet(input_shape, pdc_config, initial_filters, num_stages, blocks_per_stage, num_classes):\n",
    "    x_in = layers.Input(shape=input_shape)\n",
    "    pdc_idx = 0\n",
    "\n",
    "    # Initial Block\n",
    "    init_pdc_type = pdc_config[pdc_idx]\n",
    "    pdc_params = get_pdc_params(init_pdc_type)\n",
    "    x = layers.Conv2D(initial_filters,\n",
    "                      kernel_size=pdc_params['kernel_size'],\n",
    "                      strides=1,\n",
    "                      padding=pdc_params['padding'],\n",
    "                      use_bias=False)(x_in)\n",
    "    # x = layers.BatchNormalization()(x) # Add BN/ReLU if mimicking full structure\n",
    "    # x = layers.ReLU()(x)\n",
    "    pdc_idx += 1\n",
    "\n",
    "    current_filters = initial_filters\n",
    "    stage_outputs = []\n",
    "\n",
    "    # Stages and Blocks\n",
    "    for stage in range(num_stages):\n",
    "        for block in range(blocks_per_stage):\n",
    "            stride = 2 if stage > 0 and block == 0 else 1\n",
    "            output_filters = current_filters * 2 if stride > 1 else current_filters\n",
    "            \n",
    "            if pdc_idx >= len(pdc_config):\n",
    "                 raise IndexError(f\"PDC config list too short. Need at least {pdc_idx+1} entries.\")\n",
    "            \n",
    "            x = PDCBlock_converted_keras(x, pdc_config[pdc_idx], current_filters, output_filters, stride)\n",
    "            current_filters = output_filters\n",
    "            pdc_idx += 1\n",
    "        stage_outputs.append(x)\n",
    "\n",
    "    # Fusion Layers (Simplified Example)\n",
    "    fused_outputs = []\n",
    "    target_h, target_w = input_shape[0], input_shape[1]\n",
    "\n",
    "    for i, stage_out in enumerate(stage_outputs):\n",
    "        # Simplified: Reduce channels, then upsample\n",
    "        # In full PiDiNet, CDCM/CSAM might be here\n",
    "        reduced = layers.Conv2D(1, kernel_size=1, padding='same', use_bias=False, name=f'reduce_{i}')(stage_out)\n",
    "        \n",
    "        # Upsample to original input size\n",
    "        # Check if upsampling is needed\n",
    "        _, h, w, _ = reduced.shape\n",
    "        if h != target_h or w != target_w:\n",
    "             upsampled = layers.UpSampling2D(size=(target_h // h, target_w // w), interpolation='bilinear', name=f'upsample_{i}')(reduced)\n",
    "        else:\n",
    "             upsampled = reduced\n",
    "        fused_outputs.append(upsampled)\n",
    "\n",
    "    # Concatenate side outputs\n",
    "    if len(fused_outputs) > 1:\n",
    "        concatenated = layers.Concatenate(axis=-1)(fused_outputs)\n",
    "    else:\n",
    "        concatenated = fused_outputs[0]\n",
    "\n",
    "    # Final Classifier\n",
    "    output = layers.Conv2D(num_classes, kernel_size=1, padding='same', use_bias=False, name='classifier')(concatenated)\n",
    "    output = layers.Activation('sigmoid', name='output_sigmoid')(output)\n",
    "\n",
    "    model = models.Model(inputs=x_in, outputs=output, name='simplified_keras_pidinet')\n",
    "    return model\n",
    "\n",
    "# --- Instantiate and Verify ---\n",
    "keras_model = build_simplified_keras_pidinet(INPUT_SHAPE, PDC_CONFIG, INITIAL_FILTERS, NUM_STAGES, BLOCKS_PER_STAGE, NUM_CLASSES)\n",
    "keras_model.summary()\n",
    "\n",
    "# Verify layer sizes (optional but recommended)\n",
    "print(\"\\nChecking Layer Sizes (Trainable Parameters):\")\n",
    "for layer in keras_model.layers:\n",
    "    if layer.count_params() > 0:\n",
    "        layersize = layer.count_params()\n",
    "        print(f\"{layer.name}: {layersize}\")\n",
    "        # Example check for hls4ml latency strategy limit\n",
    "        # Note: This check is approximate. Actual hls4ml limits depend on specific layer types and configurations.\n",
    "        # The 4096 limit often applies to weights * activations product in dense/conv.\n",
    "        # Parameter count is a rough proxy.\n",
    "        if layersize > 4096*10: # Using a larger proxy limit for parameter count\n",
    "            print(f\"  WARNING: Layer {layer.name} might be large ({layersize} params) for hls4ml 'Latency' strategy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb96c84",
   "metadata": {},
   "source": [
    "## Cell 3: Data Loading and Preprocessing\n",
    "\n",
    "Load a suitable dataset (like BSDS500) and preprocess it. Ensure labels are binary (0 or 1) and match the model output shape (H, W, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a342286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Loading Parameters ---\n",
    "BATCH_SIZE = 16\n",
    "BUFFER_SIZE = 1000\n",
    "# Using a dummy dataset generator for demonstration\n",
    "# Replace with actual tfds loading (e.g., 'bsds500') or custom data pipeline\n",
    "\n",
    "def dummy_data_generator(num_samples, img_shape, num_classes):\n",
    "    \"\"\"Generates dummy image and mask data.\"\"\"\n",
    "    for _ in range(num_samples):\n",
    "        img = tf.random.uniform(img_shape, minval=0, maxval=1)\n",
    "        # Generate binary mask\n",
    "        mask = tf.random.uniform(img_shape[:2] + (num_classes,), minval=0, maxval=2, dtype=tf.int32)\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "        yield img, mask\n",
    "\n",
    "def preprocess(image, label):\n",
    "    # Normalize image if not already done\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    # Ensure label is float32 and has channel dimension\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    if len(label.shape) == 2:\n",
    "        label = label[..., tf.newaxis]\n",
    "    return image, label\n",
    "\n",
    "# --- Create Datasets ---\n",
    "output_signature = (\n",
    "    tf.TensorSpec(shape=INPUT_SHAPE, dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=INPUT_SHAPE[:2] + (NUM_CLASSES,), dtype=tf.float32)\n",
    ")\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: dummy_data_generator(100, INPUT_SHAPE, NUM_CLASSES), # 100 dummy train samples\n",
    "    output_signature=output_signature\n",
    ")\n",
    "train_dataset = train_dataset.map(preprocess).shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: dummy_data_generator(20, INPUT_SHAPE, NUM_CLASSES), # 20 dummy validation samples\n",
    "    output_signature=output_signature\n",
    ")\n",
    "val_dataset = val_dataset.map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: dummy_data_generator(20, INPUT_SHAPE, NUM_CLASSES), # 20 dummy test samples\n",
    "    output_signature=output_signature\n",
    ")\n",
    "test_dataset = test_dataset.map(preprocess).batch(BATCH_SIZE)\n",
    "\n",
    "print(f\"Train dataset: {train_dataset.element_spec}\")\n",
    "print(f\"Validation dataset: {val_dataset.element_spec}\")\n",
    "print(f\"Test dataset: {test_dataset.element_spec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d583aaae",
   "metadata": {},
   "source": [
    "## Cell 4: Floating-Point Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1d5144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Parameters ---\n",
    "EPOCHS = 5 # Use more epochs for real training\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# --- Compile and Train ---\n",
    "keras_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                    metrics=['accuracy']) # Use appropriate metrics for edge detection (e.g., IoU, F1-score)\n",
    "\n",
    "print(\"\\nTraining Keras float model...\")\n",
    "history = keras_model.fit(train_dataset,\n",
    "                          epochs=EPOCHS,\n",
    "                          validation_data=val_dataset)\n",
    "\n",
    "# --- Plot History ---\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Save Model ---\n",
    "keras_model.save('simplified_pidinet_float.h5')\n",
    "print(\"Saved Keras float model to simplified_pidinet_float.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c229504",
   "metadata": {},
   "source": [
    "## Cell 5: Define QKeras PiDiNet Architecture (Vanilla Structure)\n",
    "\n",
    "This model mirrors the Keras float model structure but uses QKeras layers (`QConv2D`, `QActivation`). The `kernel_size` and `padding` for `QConv2D` are again determined by the original PDC type. We specify quantizers for weights and activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d4ba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qkeras import autoqkeras\n",
    "# Removed incorrect import: from qkeras.qtools.quantized_operators import QuantAdd\n",
    "# --- Quantizers ---\n",
    "# Example: 6-bit weights, 6-bit activations\n",
    "WEIGHT_QUANTIZER = \"quantized_bits(6,0,alpha=1)\"\n",
    "ACT_QUANTIZER = \"quantized_relu(6)\" # ReLU activation\n",
    "BIAS_QUANTIZER = \"quantized_bits(6,0,alpha=1)\" # If using bias\n",
    "REQUANT_QUANTIZER = \"quantized_bits(6)\" # Generic quantizer for intermediate steps\n",
    "\n",
    "# --- QKeras PDC Block (Converted Structure) ---\n",
    "def QuantPDCBlock_keras(x, original_pdc_type, inplane, outplane, stride=1, weight_quantizer=None, act_quantizer=None, bias_quantizer=None):\n",
    "    \"\"\"Implements the PDC block using standard QKeras layers.\"\"\"\n",
    "    identity = x\n",
    "\n",
    "    # Shortcut connection\n",
    "    if stride > 1:\n",
    "        x_pooled = layers.MaxPooling2D(pool_size=2, strides=2)(x) # Outputs float\n",
    "        # Requantize after MaxPool before shortcut conv\n",
    "        x_requant = QActivation(REQUANT_QUANTIZER, name=f'requant_pool_{x.name.split(\"/\")[0]}')(x_pooled)\n",
    "        identity = QConv2D(outplane, kernel_size=1, strides=1, padding='same',\n",
    "                           kernel_quantizer=weight_quantizer,\n",
    "                           bias_quantizer=bias_quantizer,\n",
    "                           use_bias=False)(x_requant) # Use requantized input\n",
    "        x = x_requant # Use requantized tensor for main path as well\n",
    "    elif inplane != outplane:\n",
    "        identity = QConv2D(outplane, kernel_size=1, strides=1, padding='same',\n",
    "                           kernel_quantizer=weight_quantizer,\n",
    "                           bias_quantizer=bias_quantizer,\n",
    "                           use_bias=False)(x)\n",
    "    # else: identity remains x (already QuantTensor or requantized)\n",
    "\n",
    "    # Main path\n",
    "    pdc_params = get_pdc_params(original_pdc_type)\n",
    "    # Depthwise-like conv\n",
    "    y = QConv2D(inplane,\n",
    "                kernel_size=pdc_params['kernel_size'],\n",
    "                strides=1,\n",
    "                padding=pdc_params['padding'],\n",
    "                groups=inplane,\n",
    "                kernel_quantizer=weight_quantizer,\n",
    "                bias_quantizer=bias_quantizer,\n",
    "                use_bias=False)(x) # Input x is now always QuantTensor or requantized\n",
    "    y = QActivation(act_quantizer)(y)\n",
    "    # Pointwise conv\n",
    "    y = QConv2D(outplane, kernel_size=1, strides=1, padding='same',\n",
    "                kernel_quantizer=weight_quantizer,\n",
    "                bias_quantizer=bias_quantizer,\n",
    "                use_bias=False)(y)\n",
    "    # Apply activation BEFORE residual add\n",
    "    y_activated = QActivation(act_quantizer)(y)\n",
    "\n",
    "    # Residual connection: Wrap standard add in QActivation\n",
    "    # This ensures the output of the addition is immediately quantized\n",
    "    out = QActivation(REQUANT_QUANTIZER, name=f'requant_add_{x.name.split(\"/\")[0]}')(\n",
    "        layers.add([identity, y_activated])\n",
    "    )\n",
    "\n",
    "    # Removed redundant QActivation that was previously here\n",
    "    return out\n",
    "\n",
    "# --- Define Simplified QKeras PiDiNet Model ---\n",
    "def build_simplified_qkeras_pidinet(input_shape, pdc_config, initial_filters, num_stages, blocks_per_stage, num_classes,\n",
    "                                    weight_quantizer, act_quantizer, bias_quantizer):\n",
    "    x_in = layers.Input(shape=input_shape)\n",
    "    pdc_idx = 0\n",
    "\n",
    "    # Input Quantization (optional, depends on hls4ml config)\n",
    "    # x = QActivation('quantized_bits(8,0)')(x_in)\n",
    "    x = x_in # Assume input is float or handled by hls4ml\n",
    "\n",
    "    # Initial Block\n",
    "    init_pdc_type = pdc_config[pdc_idx]\n",
    "    pdc_params = get_pdc_params(init_pdc_type)\n",
    "    # Add input quantizer before first QConv2D if input is float\n",
    "    x_quant_in = QActivation(REQUANT_QUANTIZER, name='quant_input')(x)\n",
    "    x = QConv2D(initial_filters,\n",
    "                kernel_size=pdc_params['kernel_size'],\n",
    "                strides=1,\n",
    "                padding=pdc_params['padding'],\n",
    "                kernel_quantizer=weight_quantizer,\n",
    "                bias_quantizer=bias_quantizer,\n",
    "                use_bias=False)(x_quant_in)\n",
    "    x = QActivation(act_quantizer)(x)\n",
    "    pdc_idx += 1\n",
    "\n",
    "    current_filters = initial_filters\n",
    "    stage_outputs = []\n",
    "\n",
    "    # Stages and Blocks\n",
    "    for stage in range(num_stages):\n",
    "        for block in range(blocks_per_stage):\n",
    "            stride = 2 if stage > 0 and block == 0 else 1\n",
    "            output_filters = current_filters * 2 if stride > 1 else current_filters\n",
    "\n",
    "            if pdc_idx >= len(pdc_config):\n",
    "                 raise IndexError(f\"PDC config list too short. Need at least {pdc_idx+1} entries.\")\n",
    "\n",
    "            x = QuantPDCBlock_keras(x, pdc_config[pdc_idx], current_filters, output_filters, stride,\n",
    "                                    weight_quantizer, act_quantizer, bias_quantizer)\n",
    "            current_filters = output_filters\n",
    "            pdc_idx += 1\n",
    "        stage_outputs.append(x)\n",
    "\n",
    "    # Fusion Layers (Simplified Example)\n",
    "    fused_outputs_requant = []\n",
    "    target_h, target_w = input_shape[0], input_shape[1]\n",
    "\n",
    "    for i, stage_out in enumerate(stage_outputs):\n",
    "        reduced = QConv2D(1, kernel_size=1, padding='same',\n",
    "                          kernel_quantizer=weight_quantizer,\n",
    "                          bias_quantizer=bias_quantizer,\n",
    "                          use_bias=False, name=f'qreduce_{i}')(stage_out)\n",
    "\n",
    "        # Upsample - standard Keras layer, output is float\n",
    "        _, h, w, _ = reduced.shape\n",
    "        if h != target_h or w != target_w:\n",
    "             upsampled_float = layers.UpSampling2D(size=(target_h // h, target_w // w), interpolation='bilinear', name=f'upsample_{i}')(reduced)\n",
    "        else:\n",
    "             upsampled_float = reduced # Still a QuantTensor if not upsampled\n",
    "        # Requantize after upsampling (or if not upsampled, ensure it's QuantTensor)\n",
    "        upsampled_requant = QActivation(REQUANT_QUANTIZER, name=f'requant_upsample_{i}')(upsampled_float)\n",
    "        fused_outputs_requant.append(upsampled_requant)\n",
    "\n",
    "    # Concatenate side outputs (inputs are now QuantTensors)\n",
    "    if len(fused_outputs_requant) > 1:\n",
    "        # Concatenate might still output float if inputs have different scales\n",
    "        concatenated_maybe_float = layers.Concatenate(axis=-1)(fused_outputs_requant)\n",
    "    else:\n",
    "        concatenated_maybe_float = fused_outputs_requant[0]\n",
    "\n",
    "    # Requantize after Concatenate before classifier\n",
    "    concatenated_requant = QActivation(REQUANT_QUANTIZER, name='requant_concat')(concatenated_maybe_float)\n",
    "\n",
    "    # Final Classifier\n",
    "    output = QConv2D(num_classes, kernel_size=1, padding='same',\n",
    "                     kernel_quantizer=weight_quantizer,\n",
    "                     bias_quantizer=bias_quantizer,\n",
    "                     use_bias=False, name='qclassifier')(concatenated_requant) # Input is requantized\n",
    "    # Final activation - Use QActivation sigmoid\n",
    "    output = QActivation('quantized_sigmoid(6)', name='output_sigmoid')(output)\n",
    "\n",
    "    model = models.Model(inputs=x_in, outputs=output, name='simplified_qkeras_pidinet')\n",
    "    return model\n",
    "\n",
    "# --- Instantiate and Verify ---\n",
    "qkeras_model = build_simplified_qkeras_pidinet(INPUT_SHAPE, PDC_CONFIG, INITIAL_FILTERS, NUM_STAGES, BLOCKS_PER_STAGE, NUM_CLASSES,\n",
    "                                             WEIGHT_QUANTIZER, ACT_QUANTIZER, BIAS_QUANTIZER)\n",
    "qkeras_model.summary()\n",
    "\n",
    "print(\"\\nQKeras Model Quantization Summary:\")\n",
    "autoqkeras.utils.print_qmodel_summary(qkeras_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dce28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test Cell for QuantPDCBlock_keras ---\n",
    "print(\"\\n--- Testing QuantPDCBlock_keras in Isolation (Identity vs Main Path) ---\")\n",
    "\n",
    "# Check for GPU\n",
    "print(\"Available GPUs:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Test Parameters (remain the same)\n",
    "test_batch_size = 2\n",
    "test_h, test_w = 32, 32\n",
    "test_in_channels = 16\n",
    "test_out_channels_same = 16 # For stride=1, same channels\n",
    "test_out_channels_diff = 32 # For stride=2, different channels\n",
    "test_stride_1 = 1\n",
    "test_stride_2 = 2\n",
    "test_pdc_type = 'cd'\n",
    "\n",
    "# Dummy Input Data (Float)\n",
    "dummy_input_shape = (test_batch_size, test_h, test_w, test_in_channels)\n",
    "dummy_input_data = tf.random.normal(dummy_input_shape)\n",
    "print(f\"Dummy Input Shape: {dummy_input_data.shape}\")\n",
    "\n",
    "# --- Test 1: Identity Path Only ---\n",
    "print(\"\\n--- Testing Identity Path Only ---\")\n",
    "# Case 1a: Stride = 1, Same Channels (Identity should be just input quantization)\n",
    "input_tensor_id1a = layers.Input(shape=(test_h, test_w, test_in_channels))\n",
    "quant_input_id1a = QActivation(REQUANT_QUANTIZER, name='id1a_quant_input')(input_tensor_id1a)\n",
    "# In QuantPDCBlock, if stride=1 and channels same, identity = x (which is already quant)\n",
    "# Replace qkeras.QuantizedIdentity with standard Keras Identity\n",
    "identity_output_1a = layers.Identity(name='id1a_identity_passthrough')(quant_input_id1a)\n",
    "test_model_identity_1a = models.Model(inputs=input_tensor_id1a, outputs=identity_output_1a, name='IdentityPath_S1_SameC')\n",
    "# Use explicit shape for dummy labels\n",
    "dummy_labels_1a_shape = (test_batch_size, test_h, test_w, test_in_channels)\n",
    "dummy_labels_1a = tf.random.uniform(dummy_labels_1a_shape, minval=0, maxval=1)\n",
    "\n",
    "# Case 1b: Stride = 1, Different Channels\n",
    "input_tensor_id1b = layers.Input(shape=(test_h, test_w, test_in_channels))\n",
    "quant_input_id1b = QActivation(REQUANT_QUANTIZER, name='id1b_quant_input')(input_tensor_id1b)\n",
    "# Replace QConv2D with standard Conv2D for gradient check\n",
    "identity_output_1b = layers.Conv2D(test_out_channels_diff, kernel_size=1, strides=1, padding='same',\n",
    "                                  # kernel_quantizer=WEIGHT_QUANTIZER,\n",
    "                                  # bias_quantizer=BIAS_QUANTIZER,\n",
    "                                  use_bias=False, name='id1b_shortcut_conv')(quant_input_id1b)\n",
    "test_model_identity_1b = models.Model(inputs=input_tensor_id1b, outputs=identity_output_1b, name='IdentityPath_S1_DiffC')\n",
    "# Use explicit shape for dummy labels\n",
    "dummy_labels_1b_shape = (test_batch_size, test_h, test_w, test_out_channels_diff)\n",
    "dummy_labels_1b = tf.random.uniform(dummy_labels_1b_shape, minval=0, maxval=1)\n",
    "\n",
    "# Case 1c: Stride = 2 (Implies Different Channels)\n",
    "input_tensor_id1c = layers.Input(shape=(test_h, test_w, test_in_channels))\n",
    "quant_input_id1c = QActivation(REQUANT_QUANTIZER, name='id1c_quant_input')(input_tensor_id1c)\n",
    "x_pooled_id1c = layers.MaxPooling2D(pool_size=2, strides=2, name='id1c_pool')(quant_input_id1c)\n",
    "x_requant_id1c = QActivation(REQUANT_QUANTIZER, name='id1c_requant_pool')(x_pooled_id1c)\n",
    "# Replace QConv2D with standard Conv2D for gradient check\n",
    "identity_output_1c = layers.Conv2D(test_out_channels_diff, kernel_size=1, strides=1, padding='same',\n",
    "                                  # kernel_quantizer=WEIGHT_QUANTIZER,\n",
    "                                  # bias_quantizer=BIAS_QUANTIZER,\n",
    "                                  use_bias=False, name='id1c_shortcut_conv')(x_requant_id1c)\n",
    "test_model_identity_1c = models.Model(inputs=input_tensor_id1c, outputs=identity_output_1c, name='IdentityPath_S2_DiffC')\n",
    "# Use explicit shape for dummy labels (consider pooling effect)\n",
    "dummy_labels_1c_shape = (test_batch_size, test_h // 2, test_w // 2, test_out_channels_diff)\n",
    "dummy_labels_1c = tf.random.uniform(dummy_labels_1c_shape, minval=0, maxval=1)\n",
    "\n",
    "# Function to run gradient check\n",
    "def check_gradients(model_to_check, input_data, label_data, model_name):\n",
    "    print(f\"\\n--- Gradient Check for: {model_name} ---\")\n",
    "    model_to_check.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss='mse', run_eagerly=True)\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model_to_check(input_data, training=True)\n",
    "        loss = model_to_check.compiled_loss(label_data, predictions, regularization_losses=model_to_check.losses)\n",
    "    print(f\"Loss: {loss}\")\n",
    "    if loss is None or tf.math.is_nan(loss):\n",
    "        print(\"Error: Loss is None or NaN.\")\n",
    "        return\n",
    "    trainable_vars = model_to_check.trainable_variables\n",
    "    if not trainable_vars:\n",
    "        print(\"Warning: No trainable variables found.\")\n",
    "        return\n",
    "    gradients = tape.gradient(loss, trainable_vars)\n",
    "    print(\"Gradients:\")\n",
    "    all_none = True\n",
    "    none_vars = []\n",
    "    for var, grad in zip(trainable_vars, gradients):\n",
    "        if grad is not None:\n",
    "            print(f\"  Var: {var.name}, Grad Shape: {grad.shape}\")\n",
    "            all_none = False\n",
    "        else:\n",
    "            print(f\"  Var: {var.name}, Grad: None\")\n",
    "            none_vars.append(var.name)\n",
    "    if all_none:\n",
    "        print(\"Result: All gradients are None.\")\n",
    "    elif none_vars:\n",
    "        print(f\"Result: Some gradients are None: {none_vars}\")\n",
    "    else:\n",
    "        print(\"Result: All gradients computed.\")\n",
    "\n",
    "# Run checks for identity paths\n",
    "check_gradients(test_model_identity_1a, dummy_input_data, dummy_labels_1a, test_model_identity_1a.name)\n",
    "check_gradients(test_model_identity_1b, dummy_input_data, dummy_labels_1b, test_model_identity_1b.name)\n",
    "check_gradients(test_model_identity_1c, dummy_input_data, dummy_labels_1c, test_model_identity_1c.name)\n",
    "\n",
    "# --- Test 2: Main Path Only ---\n",
    "print(\"\\n--- Testing Main Path Only ---\")\n",
    "pdc_params = get_pdc_params(test_pdc_type) # Use helper from Cell 2\n",
    "input_tensor_main = layers.Input(shape=(test_h, test_w, test_in_channels))\n",
    "quant_input_main = QActivation(REQUANT_QUANTIZER, name='main_quant_input')(input_tensor_main)\n",
    "# Replace QConv2D with standard Conv2D for gradient check\n",
    "y_main = layers.Conv2D(test_in_channels, kernel_size=1, strides=1, padding='same', # DEBUG 1x1\n",
    "                      # groups=test_in_channels, # Grouped conv commented out\n",
    "                      # kernel_quantizer=WEIGHT_QUANTIZER,\n",
    "                      # bias_quantizer=BIAS_QUANTIZER,\n",
    "                      use_bias=False, name='main_conv1')(quant_input_main)\n",
    "y_main = QActivation(ACT_QUANTIZER, name='main_act1')(y_main)\n",
    "# Replace QConv2D with standard Conv2D for gradient check\n",
    "y_main = layers.Conv2D(test_out_channels_same, kernel_size=1, strides=1, padding='same',\n",
    "                      # kernel_quantizer=WEIGHT_QUANTIZER,\n",
    "                      # bias_quantizer=BIAS_QUANTIZER,\n",
    "                      use_bias=False, name='main_conv2')(y_main)\n",
    "main_path_output = QActivation(ACT_QUANTIZER, name='main_act2')(y_main)\n",
    "test_model_main = models.Model(inputs=input_tensor_main, outputs=main_path_output, name='MainPath')\n",
    "# Use explicit shape for dummy labels\n",
    "dummy_labels_main_shape = (test_batch_size, test_h, test_w, test_out_channels_same)\n",
    "dummy_labels_main = tf.random.uniform(dummy_labels_main_shape, minval=0, maxval=1)\n",
    "\n",
    "# Run check for main path\n",
    "check_gradients(test_model_main, dummy_input_data, dummy_labels_main, test_model_main.name)\n",
    "\n",
    "print(\"\\n--- Isolation Test Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bed90e",
   "metadata": {},
   "source": [
    "## Cell 6: Quantization-Aware Training (QAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8e1028",
   "metadata": {},
   "source": [
    "**Important Note:** Training this specific QKeras model requires a **GPU**. The `QuantPDCBlock_keras` uses `QConv2D` with `groups=inplane` to mimic depthwise convolutions. TensorFlow's CPU backend does not support gradient calculations for grouped convolutions, leading to an `InvalidArgumentError`. Ensure a GPU is available and configured correctly (as attempted in Cell 1) before running the training cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d817bc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compile and Train QKeras Model ---\n",
    "qkeras_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                     loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                     metrics=['accuracy'],\n",
    "                     run_eagerly=True) # Set run_eagerly=True if encountering graph mode issues\n",
    "\n",
    "print(\"\\nTraining QKeras model...\")\n",
    "# Check if GPU is available before training\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"\\nWARNING: No GPU detected. QKeras training with grouped convolutions requires a GPU.\")\n",
    "    print(\"Skipping QKeras training.\")\n",
    "    # Optionally, load a pre-trained model or skip subsequent steps\n",
    "    # qkeras_model = qkeras.utils.load_qmodel('path/to/pretrained_qkeras.h5')\n",
    "else:\n",
    "    q_history = qkeras_model.fit(train_dataset,\n",
    "                               epochs=EPOCHS,\n",
    "                               validation_data=val_dataset)\n",
    "\n",
    "    # --- Plot History ---\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(q_history.history['loss'], label='Training Loss')\n",
    "    plt.plot(q_history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('QKeras Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(q_history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(q_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('QKeras Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Save Model ---\n",
    "    qkeras_model.save('simplified_pidinet_qkeras.h5')\n",
    "    print(\"Saved QKeras model to simplified_pidinet_qkeras.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8339a866",
   "metadata": {},
   "source": [
    "## Cell 7: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c2a5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Models ---\n",
    "print(\"\\nLoading models for evaluation...\")\n",
    "loaded_keras_model = tf.keras.models.load_model('simplified_pidinet_float.h5')\n",
    "\n",
    "# Need custom objects for QKeras layers - Use load_qmodel instead\n",
    "# custom_objects = qkeras.utils.get_quantizer_custom_objects()\n",
    "# custom_objects.update(qkeras.utils.get_auto_custom_objects())\n",
    "# loaded_qkeras_model = tf.keras.models.load_model('simplified_pidinet_qkeras.h5', custom_objects=custom_objects)\n",
    "loaded_qkeras_model = qkeras.utils.load_qmodel('simplified_pidinet_qkeras.h5')\n",
    "\n",
    "print(\"Evaluating Keras float model...\")\n",
    "loss_float, acc_float = loaded_keras_model.evaluate(test_dataset, verbose=0)\n",
    "print(f\"  Float Model - Loss: {loss_float:.4f}, Accuracy: {acc_float:.4f}\")\n",
    "\n",
    "print(\"Evaluating QKeras model...\")\n",
    "# Evaluate the loaded QKeras model\n",
    "loss_q, acc_q = loaded_qkeras_model.evaluate(test_dataset, verbose=0)\n",
    "print(f\"  QKeras Model - Loss: {loss_q:.4f}, Accuracy: {acc_q:.4f}\")\n",
    "\n",
    "# --- Visualize Predictions (Example) ---\n",
    "print(\"\\nVisualizing predictions...\")\n",
    "for images, labels in test_dataset.take(1):\n",
    "    pred_float = loaded_keras_model.predict(images)\n",
    "    pred_q = loaded_qkeras_model.predict(images)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    n_show = min(BATCH_SIZE, 4)\n",
    "    for i in range(n_show):\n",
    "        # Original Image\n",
    "        plt.subplot(3, n_show, i + 1)\n",
    "        plt.imshow(images[i])\n",
    "        plt.title(\"Input\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Float Prediction\n",
    "        plt.subplot(3, n_show, i + 1 + n_show)\n",
    "        plt.imshow(pred_float[i][..., 0], cmap='gray') # Assuming single channel output\n",
    "        plt.title(\"Float Pred\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        # QKeras Prediction\n",
    "        plt.subplot(3, n_show, i + 1 + 2 * n_show)\n",
    "        plt.imshow(pred_q[i][..., 0], cmap='gray')\n",
    "        plt.title(\"QKeras Pred\")\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978e9a1b",
   "metadata": {},
   "source": [
    "### Strategy: Training on Newer TF, Converting on Older TF\n",
    "\n",
    "If you encounter GPU compatibility issues with a specific TensorFlow version required for a later step (like hls4ml conversion), you can try the following:\n",
    "\n",
    "1.  **Train:** Use an environment (e.g., default Colab) with a newer TensorFlow version where the GPU works correctly. Train your model (float or QKeras) and save it using `model.save('my_model_trained.h5')`.\n",
    "2.  **Switch Environment:** Start a new environment and install the required older TensorFlow version (e.g., `pip install \"tensorflow==2.14.0\"`). Also install the *exact same versions* of QKeras and any other relevant libraries (like `tensorflow-model-optimization`) used during training.\n",
    "3.  **Load:** Load the saved model in the older TF environment using `tf.keras.models.load_model('my_model_trained.h5', custom_objects=...)`. Remember to provide the necessary `custom_objects` for QKeras/pruning layers.\n",
    "4.  **Convert/Synthesize:** Proceed with the steps requiring the older TF version (e.g., hls4ml conversion below).\n",
    "\n",
    "**Caveats:**\n",
    "*   **Forward Compatibility:** Loading a model saved in a newer TF version into an older one might fail if the model uses features not present in the older version.\n",
    "*   **Dependency Versions:** Ensure library versions (QKeras, etc.) are compatible across both TF environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3931fcb",
   "metadata": {},
   "source": [
    "## Cell 8: hls4ml Conversion\n",
    "\n",
    "Now, we convert the trained QKeras model to an hls4ml project. \n",
    "\n",
    "**Important:** Because the QKeras model was constructed using the target vanilla CNN structure (with kernel sizes/padding reflecting the original PDC conversion logic), no separate weight/model conversion step (like the original PiDiNet's `convert_pidinet.py`) is required before hls4ml conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ef49c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Trained QKeras Model ---\n",
    "print(\"\\nLoading QKeras model for hls4ml conversion...\")\n",
    "custom_objects = qkeras.utils.get_quantizer_custom_objects()\n",
    "custom_objects.update(qkeras.utils.get_auto_custom_objects())\n",
    "qmodel_for_hls = tf.keras.models.load_model('simplified_pidinet_qkeras.h5', custom_objects=custom_objects)\n",
    "\n",
    "# --- Define hls4ml Configuration ---\n",
    "hls_config = hls4ml.utils.config_from_keras_model(qmodel_for_hls,\n",
    "                                                 granularity='name', # Layer-by-layer config\n",
    "                                                 backend='Vitis', # Or Vivado\n",
    "                                                 # Default precision if not inferred from QKeras:\n",
    "                                                 # default_precision='ap_fixed<16,6>'\n",
    "                                                )\n",
    "\n",
    "# **Crucial for CNNs:** Use streaming IO\n",
    "hls_config['IOType'] = 'io_stream'\n",
    "\n",
    "# Set Strategy (Latency for max parallelism, Resource for lower resources)\n",
    "# Check layer sizes from Cell 2 - if any are very large, 'Resource' might be needed\n",
    "hls_config['Model']['Strategy'] = 'Latency'\n",
    "\n",
    "# Specify target device (Update with your actual device)\n",
    "XILINX_PART = 'xcu250-figd2104-2L-e' # Example UltraScale+\n",
    "# XILINX_PART = 'xcvu9p-flgb2104-2L-e'\n",
    "# XILINX_PART = 'xc7vx690t-ffg1761-2' # Example Virtex-7\n",
    "\n",
    "# --- Print and Modify Config (Optional) ---\n",
    "print(\"\\nGenerated hls4ml config:\")\n",
    "# hls4ml.utils.print_dict(hls_config)\n",
    "\n",
    "# Example modification: Increase ReuseFactor for a specific large layer if needed\n",
    "# if 'qconv2d_large_layer_name' in hls_config['LayerName']:\n",
    "#    hls_config['LayerName']['qconv2d_large_layer_name']['ReuseFactor'] = 4\n",
    "#    hls_config['LayerName']['qconv2d_large_layer_name']['Strategy'] = 'Resource'\n",
    "\n",
    "# --- Convert to hls4ml Model ---\n",
    "output_dir = 'hls4ml_simplified_pidinet_prj'\n",
    "print(f\"\\nConverting to hls4ml project in: {output_dir}\")\n",
    "\n",
    "hls_model = hls4ml.converters.convert_from_keras_model(\n",
    "    qmodel_for_hls,\n",
    "    hls_config=hls_config,\n",
    "    output_dir=output_dir,\n",
    "    backend='Vitis', # Match backend in config\n",
    "    part=XILINX_PART\n",
    ")\n",
    "\n",
    "# --- Compile the hls4ml Model ---\n",
    "# This generates the HLS C++ code\n",
    "print(\"\\nCompiling hls4ml model...\")\n",
    "hls_model.compile()\n",
    "print(\"Compilation complete.\")\n",
    "\n",
    "# --- Optional: Build (Synthesize) the Project ---\n",
    "# This runs Vivado/Vitis HLS - can take a long time!\n",
    "run_synthesis = False\n",
    "if run_synthesis:\n",
    "    print(\"\\nRunning HLS synthesis (this may take a while)...\")\n",
    "    # csim=True runs C simulation, synth=True runs C synthesis, cosim=True runs RTL simulation\n",
    "    # vsynth=True runs Vivado synthesis for resource estimates\n",
    "    hls_model.build(csim=False, synth=True, cosim=False, validation=False, vsynth=True)\n",
    "    print(\"\\nSynthesis complete. Reports are in:\")\n",
    "    print(f\"{output_dir}/myproject_prj/solution1/syn/report/myproject_csynth.rpt\")\n",
    "    print(f\"{output_dir}/vivado_synth.rpt\")\n",
    "\n",
    "    # --- Optional: Print Reports ---\n",
    "    # hls4ml.report.read_vivado_report(output_dir)\n",
    "else:\n",
    "    print(\"\\nSkipping HLS synthesis. Project generated in:\", output_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
